## Модель RuGPT3-small

Модель RuGPT3-small — это нейросеть с 117 миллионами параметров, разработанная командой **SberDevices** на основе статьи от OpenAI и кода модели GPT-2. Команда смогла адаптировать архитектуру GPT-2, создав русскоязычный аналог под названием **RuGPT-3**, который представлен в пяти различных вариациях. Эти модели варьируются по размеру от **125 миллионов до 13 миллиардов параметров**, что позволяет использовать их для различных задач, требующих разной вычислительной мощности и точности.

### Ключевые характеристики модели:
- **Количество параметров:** 117 миллионов.
- **Вариации модели:** от 125 млн. до 13 млрд. параметров.
- **Общий объём данных:** 600 ГБ.

### Обучение и корпус данных:
Модель была обучена на больших объёмах русскоязычных текстов разных стилей, что обеспечивает её способность понимать и генерировать текст на русском языке в широком контексте. Среди источников данных были:
- **Энциклопедии**
- **Социальные сети**
- **Художественная литература**
- **Бизнес-литература**

# Влияние параметров на модель GPT

При генерации текста с помощью модели GPT параметры существенно влияют на результат. В этом файле мы разберем ключевые параметры, такие как `max_length`, `num_return_sequences`, `no_repeat_ngram_size`, `do_sample`, `top_k`, `top_p` и `temperature`, и объясним, на что они влияют.

## 1. `max_length`
- **Описание**: Максимальная длина сгенерированного текста (в токенах).
- **Влияние**: Этот параметр определяет, сколько токенов может быть сгенерировано в ответе. Чем больше значение `max_length`, тем длиннее может быть ответ. Токен — это слово или его часть, поэтому длина не всегда соответствует количеству слов.
- **Пример**:
  - При значении `max_length=50` текст будет кратким и завершенным.
  - При значении `max_length=500` модель сможет генерировать более длинные и детализированные ответы.

## 2. `num_return_sequences`
- **Описание**: Количество сгенерированных вариантов.
- **Влияние**: Указывает, сколько версий ответа вернет модель. Это полезно, если нужно несколько альтернативных ответов для выбора лучшего.
- **Пример**:
  - При `num_return_sequences=1` будет возвращен один вариант текста.
  - При `num_return_sequences=3` модель сгенерирует три различных версии ответа.

## 3. `no_repeat_ngram_size`
- **Описание**: Параметр, предотвращающий повторение фраз определенной длины.
- **Влияние**: Этот параметр задает ограничение на повторение n-грамм (фраз длиной в несколько слов). Если модель попытается повторить фразу длиной в n слов, она будет наказана за это, что минимизирует повторы.
- **Пример**:
  - При значении `no_repeat_ngram_size=2` модель избегает повторения фраз длиной в два слова подряд.

## 4. `do_sample`
- **Описание**: Включает случайный выбор слов при генерации, что делает текст более разнообразным.
- **Влияние**: Когда параметр включен (`True`), модель будет использовать сэмплирование для выбора слов, что приведет к большему разнообразию текста. Если параметр выключен, модель будет следовать самым вероятным вариантам.
- **Пример**:
  - При `do_sample=True` генерация будет более креативной и разнообразной.
  - При `do_sample=False` модель будет выбирать более детерминированные и предсказуемые ответы.

## 5. `top_k`
- **Описание**: Ограничение на количество самых вероятных вариантов.
- **Влияние**: Этот параметр ограничивает выбор модели до самых вероятных `K` вариантов для каждого слова. Чем меньше значение `top_k`, тем более сфокусированной будет генерация, но также менее разнообразной.
- **Пример**:
  - При `top_k=50` модель будет выбирать из 50 наиболее вероятных слов для каждого шага генерации.
  - При `top_k=5` текст будет значительно более предсказуемым, так как выбор слов будет ограничен.

## 6. `top_p`
- **Описание**: Определяет порог накопленной вероятности для выбора следующего слова в предложении. Вместо того чтобы ограничивать выбор фиксированным числом слов (как в top_k), top_p выбирает слова до тех пор, пока суммарная вероятность не достигнет заданного порога p.
- **Влияние**: Этот параметр управляет тем, насколько разнообразными будут сгенерированные токены. При `top_p=0.95` модель будет выбирать слова, суммарная вероятность которых равна 95%. Чем ближе значение к 1, тем более разнообразной будет генерация, и наоборот.
- **Пример**:
  - Если top_p=0.3, модель может выбрать только из нескольких самых вероятных слов, например, "хорошая", "солнечная", "дождливая".
  - Если top_p=0.9, выбор будет шире и может включать слова вроде "ветреная", "облачная" и даже редкие варианты.

## 7. `temperature`
- **Описание**: Контроль случайности в генерации.
- **Влияние**: Чем выше значение `temperature`, тем менее предсказуемыми и более разнообразными будут сгенерированные ответы. Чем ниже значение, тем более детерминированным будет результат. При `temperature=1.0` используется стандартный уровень случайности.
- **Пример**:
  - При `temperature=1.0` модель сгенерирует ответы с балансом между креативностью и предсказуемостью.
  - При `temperature=0.2` модель будет генерировать более предсказуемые и точные ответы, фокусируясь на самых вероятных вариантах.

## 8. `num_beams`
- **Описание**: `num_beams` определяет количество "лучей" или параллельных путей, которые модель будет использовать при генерации текста. Это метод, известный как "beam search".
- **Влияние**: При генерации текста модель рассматривает несколько вариантов продолжения одновременно. Каждый "луч" представляет собой отдельный путь генерации. В конце выбирается наиболее вероятный и качественный вариант.
- **Пример**:
  - `num_beams=1`: Модель выбирает самое вероятное слово на каждом шаге, что может привести к менее разнообразному, но быстрым результатам.
  - `num_beams=3`: Модель рассматривает три наиболее вероятных варианта на каждом шаге и выбирает лучший из них.
